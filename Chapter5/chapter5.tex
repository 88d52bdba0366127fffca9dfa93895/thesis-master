%!TEX root = ../thesis.tex
\chapter{K-Segments Under Bagging approach: An experimental Study on Extremely Imbalanced Data Classification}

\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

Imbalanced dataset could be found in many real-world domains of applications, e.g. fraud detection problem, threat detection, etc.,. There are various methods that have been proposed to handle the imbalanced data classification problems, but there is no guarantee those methods will work well in the case of extremely imbalanced data. Practically, we can find the explosion of imbalance issue in the case of big dataset analysis, in which the imbalance ratio increases uncontrollably. We all know that Big Data is a terminology used to express the increasing level of both volume and complexity of the data, and the big data within extreme imbalance scenario is such a research question from recent work. In this study, we propose a simplified combination of under-sampling and ensemble learning which can adapt well with different scenarios of extreme imbalance. Experimentally, we carry out our test on 17 datasets, taken from the UCI repository and Kaggle, and can show that our proposed method is not only competitive with a common method but also very effective especially in the case of extremely imbalanced big data classification problems.

\section{Introduction}

Imbalance is one common issue in many real domains, e.g. fraud detection, telecommunication. In those cases, we need to identify a small number of positive data points (minority class) stand among too many redundant data points. Consider a classification task of a dataset with imbalance ratio (IR) of 100, i.e. in every 101 samples there is only one positive sample that we need to detect. Most of algorithms in Machine Learning are not designed to handle this situation; if they maximize their accuracy then in the worst case they always have the accuracy of 99\% by doing nothing. This lazy classifier marking all samples in a dataset as majority class has very high accuracy, but mis-classify all minority samples. Many studies have reported that they lose their performance with the imbalance datasets. \cite{chen2004using,chen2005pruning,wang2006classification,hong2007kernel}.

To handle the imbalance problem, there are many proposed methods and these could be grouped into two levels: algorithmic level and data level. At the algorithmic level, classifiers are designed or modified from existing algorithms to handle the imbalanced data by itself \cite{bradford1998pruning,cieslak2008learning}. At the data level, original imbalanced data will be modified by pre-processing step before applying to normal classification algorithms, e.g. under-sampling, over-sampling \cite{drummond2003c4} or SMOTE \cite{chawla2002smote}. Several studies \cite{weiss2001effect, laurikkala2001improving, estabrooks2004multiple} have shown that a balanced training set will give the better performance when using normal algorithms. Cost-sensitive approach \cite{ling2004decision} uses a similar idea with those above approaches to minimize misclassification costs, which are higher for data points in the minority class; however, it requires side information about the cost which is not always available.

In the age of computing world, the size of data quickly increases to a huge volume due to the advantages of computer technologies. Big dataset could be seen in many domains like genome biology, banking system. The size of the dataset could be million or billion records, which leads to the question that whether our solution effective with a massive amount of data. Particularly in an imbalanced big data classification \cite{del2014use, triguero2015evolutionary, fernandez2017insight}, previous approaches could not provide solutions since they create a bigger dataset then cannot fit into our resources or has poor results. Especially, an extremely imbalanced data problem in a big dataset is another story that receives attention very recently \cite{triguero2015rosefw, krawczyk2016learning}.

Several studies in imbalanced datasets have imbalance ratio less than 100, i.e. minority class greater than 1\% of the data, and those approaches cannot guarantee the accuracy for the highly imbalanced tasks (IR $>$ 100). In some applications, the datasets are not only really huge but also highly imbalanced, e.g. fraud detection often has IR greater than 1000 \cite{juszczak2008off}. From this scenario of two difficult problems, we need an effective way to tackle both of them at the same time and therefore, in this study we want to show a simplified combination of under-sampling technique and ensemble learning could have a good result in the extremely imbalanced big data classification.

The paper is organized as follows: section 2 mentions recent works relate to the extremely imbalanced data, imbalance big data classification and the gap between them. Section 3 presents our methodology, experimental design and comparative result on many datasets. Then we finally come to a conclusion and future works in section 4.


\section{Recent works on extreme imbalance in big data classification}

In the class imbalance problem, Mikel Galar at el. \cite{galar2012review} gave a comprehensive review of several methodologies that have been proposed to deal with this problem. From their empirical comparison of the most significant published approaches, they have concluded that ensemble-based algorithms, e.g. using bagging or boosting, are worthwhile and under-sampling techniques, such as RUSBoost or UnderBagging,  could have higher performances than many other complex approaches.

In the Big Data issue, techniques used to deal with it usually based on distributed computing on a system of computers. Similarly in order to handle the imbalance problem in the big dataset, Sara del Río at el. \cite{del2014use} implemented a distributed version of those common sampling techniques based on MapReduce framework \cite{miner2012mapreduce} and they found that the under-sampling method could manage large datasets. Isaac Triguero at el. \cite{triguero2015rosefw} faced with the extremely imbalanced big data in bioinformatics problem and their solution is a complex combination of Random Forest \cite{breiman2001random} and oversampling that balanced the distribution of classes. In a review of Alberto Fernández at el. \cite{fernandez2017insight} on the Big Data and Imbalanced classification, they considered different sampling ratio between classes and stated that a perfectly balanced sample was not the best method as traditional approaches that have been used \cite{hido2009roughly, garcia2009evolutionary}.

Recently, some researchers notice the extremely imbalance problem on datasets with imbalance ratio greater than 100 \cite{tang2009svms, triguero2015rosefw}. This context easily leads to a classification task in the Big Data since the positive class is a very small fraction in the dataset, if the dataset is small then we cannot find any meaningful patterns on a few positive data points. To our best knowledge, there are a few studies on this gap of extremely imbalanced big data classification. For example, Sara del Río \cite{del2014use} tested their study on datasets with a largest size is over five million data points or a maximum IR is 74680. This new scenario raises a question about how effectiveness we could achieve with this kind of data. An ideal method need to use any given data points very effective, for both positive and negative data points.

In the traditional imbalance problem, many evaluation measures could not be used in this case independently since they are affected by the majority class. In order to evaluate the classifier, it requires a stronger metric that balance the classes and a commonly used metric for this case is $F_1$ score that can derive from confusion matrix (table \ref{tbl:confusion_matrix}). Based on two other metrics, Precision and Recall, the $F_1$ score is the harmonic average of them and could be computed as formula \ref{eq:f1}. In this study we propose to use the $F_1$ score as a main measure.


\begin{table}[h]
\centering
\caption{Confusion matrix}
\label{tbl:confusion_matrix}
\begin{tabular}{lll}
\hline
               & predicted positives & predicted negatives \\ \hline
real positives & True positive (TP)  & False negative (FN) \\ \hline
real negatives & False positive (FP) & True negative (TN)  \\ \hline
\end{tabular}
\end{table}

\begin{equation}
\label{eq:precision}
Precision = \dfrac{TP}{TP + FP}
\end{equation}

\begin{equation}
\label{eq:recall}
Recall = \dfrac{TP}{TP + FN}
\end{equation}

\begin{equation}
\label{eq:f1}
F_1 = 2 \cdot \dfrac{Precision \cdot Recall}{Precision + Recall}
\end{equation}


\section{Proposed analysis}


Based on the question posed in the previous section, we propose another way to build samples that effectively use all of the data points in the dataset once time. Using the under-sampling method, all negative data points will be randomly divided into several dis-joint same-size subsets then each subset will be merged back with all positive data points then it is used to train a classifier. After that, we combine these classifiers by the voting ensemble as a final prediction. Suppose a dataset $D$ has $N$ data points with imbalance ratio is $IR$, let $K$ be a number of subsets, our proposed approach is summarized by Algorithm \ref{alg:ksus}, in which we define $D^{major}$ and $D^{minor}$ are the majority and minority classes in the dataset $D$ respectively and $n = \begin{bmatrix} \dfrac{ \begin{vmatrix} D^{major} \end{vmatrix} }{ K } \end{bmatrix}$ is a size of a sampled subset. In the main part of Algorithm \ref{alg:ksus},  in this study we mainly use Random Forest because of its robustness and good performance.


\begin{algorithm}[h]
\caption{$K$-Segments Under Bagging ($K$-SUB)}
\label{alg:ksus}

Inputs:
\newline
$D^{major}$ is the majority class in the dataset $D$
\newline
$D^{minor}$ is the minority class in the dataset $D$
\newline
$K$ is the number of subsets

Procedure:
\begin{algorithmic}[1]
\For {$k = 1$ to $K$}:
  \State Draw a subset $K^{major}_k$ without replacement from the majority class with the size $n = \begin{bmatrix} \dfrac{ \begin{vmatrix} D^{major} \end{vmatrix} }{ K } \end{bmatrix}$
  \State Let $K_k$ is a merged subset of $K^{major}_k$ with all data points in minority class $D^{minor}$
  \State Train a classifier $f^k$ by applying Random Forest algorithm to the subset $K_k$
\EndFor
\State Combine all $f_k$ into an aggregated model $F$
\State Return F
\end{algorithmic}

% Procedure:
% \newline \nl For $k = 1$ to $K$:
% \newline \nl \qquad Draw a subset $K^{major}_k$ without replacement from the majority class with the size $n = \begin{bmatrix} \dfrac{ \begin{vmatrix} D^{major} \end{vmatrix} }{ K } \end{bmatrix}$
% \newline \nl \qquad Let $K_k$ is a merged subset of $K^{major}_k$ with all data points in minority class $D^{minor}$
% \newline \nl \qquad Train a classifier $f^k$ by applying Random Forest algorithm to the subset $K_k$
% \newline \nl Combine all $f_k$ into an aggregated model $F$
% \newline \nl Return F

\end{algorithm}

%In order to train a sub-classifier in our method, we only need the smaller size of training data than the original one. Particularly in the extreme imbalance cases with very high $IR$ we propose to use small value for $K$ compared with $IR$, i.e. $IR >> K$. While in the case of Under Bagging algorithm, in order to have the same number of negative data points used to train classifiers, it needs to duplicate minority class $IR$ times and it leads the dataset will be roughly doubled. It means that we are twice as effective than the old balanced bagging method in the final number of used data points.

To illustrate the effectiveness of our proposed algorithm, we apply it on many datasets with various aspects, e.g. from small to large datasets, from low to very high imbalance ratio and moreover we try to do our test with several values of $K$, from 3, 5, 10 to 20. We run a stratified 5-fold cross-validation then compute the metric $F_1$ score as an average of five folds and the running time is also reported in seconds. This test is carried out in one single machine with 24 cores and 128 gigabytes in memory. All details of the final results are summarized in the table \ref{tbl:summary_results}. 


% \begin{table*}[]
% \caption{Summary of datasets and the experimental results}
% \label{tbl:summary_results}
% \resizebox{\textwidth}{!}{%

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table*}[]
\caption{Summary of datasets and the experimental results}
\label{tbl:summary_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{dataset} & \multirow{2}{*}{IR} & \multirow{2}{*}{sample} & \multirow{2}{*}{feature} & \multicolumn{2}{c|}{UB}  & \multicolumn{2}{c|}{3-SUB} & \multicolumn{2}{c|}{5-SUB} & \multicolumn{2}{c|}{10-SUB} & \multicolumn{2}{c|}{20-SUB} \\ \cline{5-14} 
                         &                     &                         &                          & F1              & time   & F1                & time   & F1                & time   & F1                 & time   & F1            & time        \\ \hline
ecoli                    & 8                   & 336                     & 7                        & \textbf{0.5758} & 1      & 0.5669            & 1      & 0.4850            & 2      & 0.4563             & 5      & 0.3953        & 9           \\ \hline
spectrometer             & 10                  & 531                     & 93                       & 0.7642          & 1      & \textbf{0.8411}   & 1      & 0.8248            & 2      & 0.7933             & 4      & 0.6601        & 10          \\ \hline
car\_eval\_34            & 11                  & 1728                    & 21                       & \textbf{0.7173} & 2      & 0.4445            & 1      & 0.2418            & 2      & 0.2380             & 5      & 0.2081        & 9           \\ \hline
isolet                   & 11                  & 7797                    & 617                      & 0.6084          & 5      & \textbf{0.7702}   & 3      & 0.7326            & 4      & 0.5984             & 7      & 0.4710        & 13          \\ \hline
libras\_move             & 14                  & 360                     & 90                       & 0.6797          & 2      & \textbf{0.7389}   & 1      & 0.6473            & 2      & 0.4036             & 4      & 0.1717        & 9           \\ \hline
thyroid\_sick            & 15                  & 3772                    & 52                       & 0.7551          & 2      & 0.8023            & 2      & \textbf{0.8261}   & 3      & 0.8112             & 5      & 0.6708        & 10          \\ \hline
oil                      & 21                  & 937                     & 49                       & \textbf{0.3083} & 3      & 0.0941            & 1      & 0.0829            & 2      & 0.0848             & 4      & 0.0841        & 9           \\ \hline
car\_eval\_4             & 25                  & 1728                    & 21                       & \textbf{0.6078} & 4      & 0.4413            & 1      & 0.2183            & 2      & 0.1623             & 5      & 0.1481        & 10          \\ \hline
letter\_img              & 26                  & 20000                   & 16                       & 0.6133          & 5      & \textbf{0.8681}   & 3      & 0.8381            & 4      & 0.8059             & 9      & 0.7018        & 15          \\ \hline
webpage                  & 34                  & 344780                  & 300                      & 0.3783          & 14     & 0.1842            & 5      & 0.3130            & 6      & \textbf{0.4496}    & 12     & 0.1616        & 18          \\ \hline
mammography              & 42                  & 11183                   & 6                        & 0.4033          & 5      & 0.6478            & 1      & \textbf{0.6606}   & 1      & 0.6316             & 3      & 0.5221        & 5           \\ \hline
protein\_homo            & 111                 & 145751                  & 74                       & 0.4877          & 24     & 0.7908            & 7      & 0.7953            & 7      & \textbf{0.8125}    & 12     & 0.8113        & 15          \\ \hline
10\% kddcup R2L          & 443                 & 494021                  & 41                       & 0.2396          & 643    & 0.8945            & 44     & \textbf{0.9350}   & 46     & 0.8803             & 90     & 0.7176        & 101         \\ \hline
fraud\_detection         & 577                 & 284807                  & 29                       & 0.2341          & 148    & \textbf{0.8113}   & 30     & 0.6231            & 30     & 0.5567             & 55     & 0.3304        & 62          \\ \hline
kdd SF PROBE             & 7988                & 703067                  & 30                       & 0.0126          & 3980   & 0.7895            & 80     & 0.8034            & 79     & \textbf{0.8242}    & 156    & 0.7176        & 156         \\ \hline
10\% kdd U2R             & 9499                & 494021                  & 41                       & 0.0075          & 7008   & \textbf{0.6034}   & 38     & \textbf{0.6034}   & 39     & 0.5782             & 79     & 0.5291        & 90          \\ \hline
kdd U2R                  & 94199               & 4940219                 & 41                       & 0.0007          & 112285 & 0.3741            & 359    & 0.4571            & 343    & \textbf{0.5617}    & 737    & 0.4916        & 780         \\ \hline
\end{tabular}%
}
\end{table*}


The results in bold indicate the best $F_1$ scores from the Under Bagging (UB) algorithm and our method $K$-Segments Under Bagging ($K$-SUB) with multiple $K$ values. With datasets have low imbalance ratio (e.g. IR $<$ 50) the Under Bagging algorithm could handle the problem, and our method has also comparative results. However, from a high to extreme imbalance cases, i.e. IR $>=$ 50, our method outperform the Under Bagging in all cases with any value of our chosen $K$ values. Furthermore, the running time is dramatically slow for the case of Under Bagging algorithm, but with our method it only takes us few minutes in the same machine. Specially in the two biggest datasets with highest $IR$, the $F_1$ scores of the Under Bagging tend to become zero but our method still keeps the high $F_1$ scores. In the range of $K$ values selected above, we observe the case $K = 20$ is not the best one, so it suggests that only a small enough value of $K$ may be a good choice in our proposed analysis.


\section{Conclusion}

In this study, we have presented the $K$-Segments Under Bagging algorithm in the attempt to tackle the problem of extremely imbalanced data classification. The experimental results show that in the case of extremely imbalanced data, our method not only outperforms the previous method but also runs very fast. While in case of low imbalanced data, with the suitable $K$ value, the $K$-SUB algorithm is still useful. Furthermore, it can be observed experimentally that we may obtain the good results for the small enough values of $K$, which suggests that in real applications we only have to tune the parameter $K$ in a small range of value. With this new and challenging scenario, we have shown that the simplified combination of undersampling technique and ensemble learning is able to give better results instead of using other complicated methods addressed in the past. In the future work, we want to investigate deeper on this approach as well as other related issues.
